{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow==1.4.0 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0rc0, 2.4.0rc1, 2.4.0rc2, 2.4.0rc3, 2.4.0rc4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.5.0rc0, 2.5.0rc1, 2.5.0rc2, 2.5.0rc3, 2.5.0, 2.5.1, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0)\n",
      "ERROR: No matching distribution found for tensorflow==1.4.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\sahr\\anaconda3\\lib\\site-packages (21.1.3)\n",
      "Collecting pip\n",
      "  Downloading pip-21.2.4-py3-none-any.whl (1.6 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.1.3\n",
      "    Uninstalling pip-21.1.3:\n",
      "      Successfully uninstalled pip-21.1.3\n",
      "Successfully installed pip-21.2.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Problem 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SAHR\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>146</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>147</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>148</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>149</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>150</td>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n",
       "0      1            5.1           3.5            1.4           0.2   \n",
       "1      2            4.9           3.0            1.4           0.2   \n",
       "2      3            4.7           3.2            1.3           0.2   \n",
       "3      4            4.6           3.1            1.5           0.2   \n",
       "4      5            5.0           3.6            1.4           0.2   \n",
       "..   ...            ...           ...            ...           ...   \n",
       "145  146            6.7           3.0            5.2           2.3   \n",
       "146  147            6.3           2.5            5.0           1.9   \n",
       "147  148            6.5           3.0            5.2           2.0   \n",
       "148  149            6.2           3.4            5.4           2.3   \n",
       "149  150            5.9           3.0            5.1           1.8   \n",
       "\n",
       "            Species  \n",
       "0       Iris-setosa  \n",
       "1       Iris-setosa  \n",
       "2       Iris-setosa  \n",
       "3       Iris-setosa  \n",
       "4       Iris-setosa  \n",
       "..              ...  \n",
       "145  Iris-virginica  \n",
       "146  Iris-virginica  \n",
       "147  Iris-virginica  \n",
       "148  Iris-virginica  \n",
       "149  Iris-virginica  \n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f4835bc05c2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"float\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"float\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "class GetMiniBatch:\n",
    "\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
    "    return layer_output\n",
    "                               \n",
    "logits = example_net(X)\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 29.7392, val_loss : 12.9915, acc : 0.000, val_acc : 0.500\n",
      "Epoch 1, loss : 0.0051, val_loss : 3.7442, acc : 1.000, val_acc : 0.625\n",
      "Epoch 2, loss : 0.0000, val_loss : 6.0118, acc : 1.000, val_acc : 0.667\n",
      "Epoch 3, loss : 0.0000, val_loss : 2.1866, acc : 1.000, val_acc : 0.833\n",
      "Epoch 4, loss : 0.0000, val_loss : 0.7863, acc : 1.000, val_acc : 0.958\n",
      "Epoch 5, loss : 0.0002, val_loss : 2.8166, acc : 1.000, val_acc : 0.917\n",
      "Epoch 6, loss : 1.5471, val_loss : 5.2585, acc : 0.833, val_acc : 0.792\n",
      "Epoch 7, loss : 0.0079, val_loss : 3.6528, acc : 1.000, val_acc : 0.917\n",
      "Epoch 8, loss : 0.0000, val_loss : 2.4689, acc : 1.000, val_acc : 0.917\n",
      "Epoch 9, loss : 0.0000, val_loss : 1.3612, acc : 1.000, val_acc : 0.917\n",
      "test_acc : 1.000\n"
     ]
    }
   ],
   "source": [
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "\n",
    "y[y=='Iris-setosa'] = 0\n",
    "y[y=='Iris-versicolor'] = 1\n",
    "y[y=='Iris-virginica'] = 2\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train)\n",
    "y_val_one_hot = enc.transform(y_val)\n",
    "y_test_one_hot = enc.transform(y_test)\n",
    "\n",
    "class GetMiniBatch:\n",
    "\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "    \n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 3\n",
    "\n",
    "X = tf.compat.v1.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.compat.v1.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
    "    return layer_output\n",
    "                             \n",
    "logits = example_net(X)\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 25.2946, val_loss : 53.0384, acc : 0.667, val_acc : 0.292\n",
      "Epoch 1, loss : 0.0003, val_loss : 4.5520, acc : 1.000, val_acc : 0.792\n",
      "Epoch 2, loss : 4.7784, val_loss : 11.2675, acc : 0.500, val_acc : 0.708\n",
      "Epoch 3, loss : 0.0000, val_loss : 2.4209, acc : 1.000, val_acc : 0.917\n",
      "Epoch 4, loss : 0.3382, val_loss : 5.1952, acc : 0.833, val_acc : 0.833\n",
      "Epoch 5, loss : 0.0000, val_loss : 1.9240, acc : 1.000, val_acc : 0.917\n",
      "Epoch 6, loss : 0.0000, val_loss : 5.5849, acc : 1.000, val_acc : 0.792\n",
      "Epoch 7, loss : 0.0000, val_loss : 3.4329, acc : 1.000, val_acc : 0.792\n",
      "Epoch 8, loss : 0.0000, val_loss : 2.1924, acc : 1.000, val_acc : 0.833\n",
      "Epoch 9, loss : 0.0000, val_loss : 1.5920, acc : 1.000, val_acc : 0.792\n",
      "test_acc : 0.967\n"
     ]
    }
   ],
   "source": [
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "\n",
    "y[y=='Iris-setosa'] = 0\n",
    "y[y=='Iris-versicolor'] = 1\n",
    "y[y=='Iris-virginica'] = 2\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train)\n",
    "y_val_one_hot = enc.transform(y_val)\n",
    "y_test_one_hot = enc.transform(y_test)\n",
    "\n",
    "class GetMiniBatch:\n",
    "\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "    \n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 3\n",
    "\n",
    "X = tf.compat.v1.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.compat.v1.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
    "    return layer_output\n",
    "                             \n",
    "logits = example_net(X)\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 393377.6250, val_loss : 283223.7500\n",
      "Epoch 1, loss : 115025.0938, val_loss : 89766.0000\n",
      "Epoch 2, loss : 83156.5547, val_loss : 72863.2031\n",
      "Epoch 3, loss : 41136.8789, val_loss : 36266.9062\n",
      "Epoch 4, loss : 22160.4883, val_loss : 14907.4697\n",
      "Epoch 5, loss : 16793.8066, val_loss : 9422.9502\n",
      "Epoch 6, loss : 19186.1465, val_loss : 14873.8828\n",
      "Epoch 7, loss : 19947.7969, val_loss : 16414.6758\n",
      "Epoch 8, loss : 14225.5156, val_loss : 9530.2178\n",
      "Epoch 9, loss : 12256.4336, val_loss : 3722.3206\n",
      "test_mse : 12256.434\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "dataset_path =\"train.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "y = df[\"SalePrice\"]\n",
    "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "y = np.log(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "class GetMiniBatch:\n",
    "\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
    "    return layer_output\n",
    "                               \n",
    "logits = example_net(X)\n",
    "loss_op =  tf.losses.mean_squared_error(labels=Y, predictions=logits)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "y_pred = logits\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    loss_list = []\n",
    "    val_loss_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "        loss = sess.run(loss_op, feed_dict={X: X_train, Y: y_train})\n",
    "        loss_list.append(loss)\n",
    "        val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
    "        val_loss_list.append(val_loss)    \n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, loss, val_loss))\n",
    "    print(\"test_mse : {:.3f}\".format(loss))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.plot(loss_list, label='loss')\n",
    "    plt.plot(val_loss_list, label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 1.3328, val_loss : 1.0855, acc : 0.400, val_acc : 0.707\n",
      "Epoch 1, loss : 1.2265, val_loss : 0.8920, acc : 0.400, val_acc : 0.769\n",
      "Epoch 2, loss : 0.7819, val_loss : 0.5105, acc : 0.700, val_acc : 0.886\n",
      "Epoch 3, loss : 0.6478, val_loss : 0.4163, acc : 0.800, val_acc : 0.904\n",
      "Epoch 4, loss : 0.5739, val_loss : 0.3662, acc : 0.700, val_acc : 0.912\n",
      "Epoch 5, loss : 0.2464, val_loss : 0.3432, acc : 0.900, val_acc : 0.915\n",
      "Epoch 6, loss : 0.5607, val_loss : 0.3240, acc : 0.800, val_acc : 0.921\n",
      "Epoch 7, loss : 0.4329, val_loss : 0.3433, acc : 0.900, val_acc : 0.921\n",
      "Epoch 8, loss : 0.3519, val_loss : 0.4270, acc : 0.900, val_acc : 0.915\n",
      "Epoch 9, loss : 0.3793, val_loss : 0.5918, acc : 0.900, val_acc : 0.918\n",
      "test_acc : 0.921\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "y_train = y_train.astype(np.int)[:, np.newaxis]\n",
    "y_test = y_test.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:])\n",
    "y_test_one_hot = enc.fit_transform(y_test[:])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "\n",
    "class GetMiniBatch:\n",
    "\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 10\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
    "    return layer_output\n",
    "                               \n",
    "logits = example_net(X)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "max_Y = (tf.argmax(Y, 1))\n",
    "max_Y_pred = tf.argmax(logits, 1)\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "    \n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
